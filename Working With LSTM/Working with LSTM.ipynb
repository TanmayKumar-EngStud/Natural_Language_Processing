{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from numpy.matrixlib.defmatrix import matrix\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "from tensorflow.python.keras.backend import categorical_crossentropy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.layers.core import Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "import numpy as np\n",
    "from pickle import dump, load"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "nlp = spacy.load('en')\n",
    "file = open('/home/tanmay/Desktop/NLP Workflow/Working with a big data/collected_data.txt','r')\n",
    "minidata = file.readline()\n",
    "club = [text.lower() for text in re.findall('[A-Za-z]+',minidata)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "tk = Tokenizer(num_words=len(club), lower= True)\n",
    "tk.fit_on_texts(club)\n",
    "sequence = tk.texts_to_sequences(club)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now making the 2D matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "training_length =math.trunc(math.sqrt(len(sequence)))\n",
    "\n",
    "new_Matrix = []\n",
    "for i in range(training_length,len(sequence)):\n",
    "    row = sequence[i-training_length:i]\n",
    "    new_Matrix.append(row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making the Matrix into a numpy array and also seperating the test case with the result set\n",
    "\n",
    "### Premodelling the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "Matrix = np.array(new_Matrix)\n",
    "vocabulary_size =len(club)\n",
    "X=Matrix[:,:-1]\n",
    "y =Matrix[:,-1]\n",
    "y = to_categorical(y, num_classes= vocabulary_size +1)\n",
    "seq_len = X.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a function whose perpose is to create our LSTM Sequential model and also compile that model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def create_model(vocab_Size, seq_Len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_Size,seq_Len,input_length=seq_Len))\n",
    "    model.add(LSTM(seq_Len*2,return_sequences=True))\n",
    "    model.add(LSTM(seq_Len*2))\n",
    "    model.add(Dense(seq_Len*2,activation='relu'))\n",
    "    model.add(Dense(vocab_Size,activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
    "    print(\"Summary :-\")\n",
    "    print(model.summary())\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the model, training it, saving it and dumping the train model into our  initialized tokenizer "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "model = create_model(vocabulary_size+ 1, seq_len)\n",
    "model.fit(X,y,batch_size = 128, verbose=2, epochs=3)\n",
    "\n",
    "model.save('Training sample.h5')\n",
    "dump(tk,open('Training sample.h5','wb'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Summary :-\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 7)              490       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 7, 14)             1232      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 14)                1624      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 70)                1050      \n",
      "=================================================================\n",
      "Total params: 4,606\n",
      "Trainable params: 4,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1/1 - 2s - loss: 4.2486 - accuracy: 0.0164\n",
      "Epoch 2/3\n",
      "1/1 - 0s - loss: 4.2476 - accuracy: 0.0492\n",
      "Epoch 3/3\n",
      "1/1 - 0s - loss: 4.2465 - accuracy: 0.0328\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the function that generate the text\n",
    "\n",
    "### Characterstics : \n",
    "- We have just train the tokenized\n",
    "- We already have knowledge about the vocabulary\n",
    "- This is robust to have shorter seen text of longer the seen text than the seuqence length\n",
    "- Seen text must be equal to the the text of model that has been used to tained on. Otherwise we have to pad it.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def generate_text(model,tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range (num_gen_words):\n",
    "        encoded_text = tokenizer.text_to_sequences([input_text])[0]\n",
    "        pad_encoding = pad_sequences([encoded_text], max_len =seq_len,truncating='pre')\n",
    "        #Here the max length is whatever the sequence length that was passed in this\n",
    "        '''\n",
    "            Moreover: This essentially makes sure that if you passed in a long seen text.\n",
    "            Thus we have taken very small set to train upon thus we have to pad it to make \n",
    "            sure that it has only small number of tokens or if the seen text happens to be too short we are going to pad it to fill uo that allocated place\n",
    "\n",
    "            In order to get better result, it is recommended just passing a seen text thatt is actually the same expected length thta model has.\n",
    "        '''\n",
    "        pred_word = model.predict_model(pad_encoding, verbose = 2)\n",
    "        input_text +=\" \"+ pred_word\n",
    "    return ' '.join(output_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using the function and then testing the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('DeepLearning': conda)"
  },
  "interpreter": {
   "hash": "ce5412b8715117908d8129964960c3be480f008a4994ece1be9c3b7cf4dabcdb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}